1. 根据已有的百科知识点，从百度上爬去one hop的知识点，然后人工筛选出与初中数学有关的知识点。
2. 根据获得的知识点，从百度百科爬去内容，然后从中找出同时包含两个或两个以上的句子，人工标注在这句句子中两个实体之间的关系，作为数据集。
3. 目前关系的类型定义为 无关、正序、逆序、同义、反义、属于、包含、属性等。
4. 以所有分词后的百科文章作为语料，使用word2vec来生成词向量。
5. 使用stanford POS工具对所有语料的句子的单词进行词性标注,并对这个特征进行embedding。
6. 使用stanford parse工具将句子解析成dependency path，然后找出shortest dependency path，将SDP上的单词喂给LSTM。
7. 对每种关系都训练专门的分类器，按照（属于、包含）-> (正序、逆序)->（属性）->（同义，反义）的顺序来构建LSTM网络。
8. 对每句句子进行预测，得到一种关系，然后将这些关系整合。或者先挑出一对实体的所有句子，然后对它们进行关系预测，最后通过投票得到最终的关系。

## 特征
1. word embedding。
2. 每个词语在整句句子中的相对位置。
4. 依赖树，这个特征目前有点问题，当一句话中目标词出现的数量≥1次的时候，Stranford生成的最短以来路径有问题。
  现在考虑对两个目标次都生成最短依赖路径，然后查找最小包含路径。
5. 每个词语的POS标签。
6. 如何突出两个目标词？
7. 是否需要用CNN提取更高维的特征？

## 实验分析
1. 数据集POS分布
> Counter({'NNP': 102572, 'NN': 6330, 'JJ': 5660, 'CD': 4035, 'VBD': 1807, '(': 876, ')': 837, 'VBZ': 820, 'DT': 506, ',': 455, ':': 424, 'VBP': 364, 'RB': 315, '$': 310, 'VB': 165, 'NNS': 133, 'MD': 115, 'SYM': 90, 'FW': 79, 'CC': 20, 'PRP': 13, 'IN': 9, 'PDT': 6, 'LS': 5, 'UH': 4, '.': 2, 'VBN': 2, 'POS': 2, "''": 1, 'TO': 1, 'NNPS': 1})

> POS的分布不是均匀的，可以考虑换成nltk的POS来标注
